{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want-To\n",
    "Use what you wrote in preProcessing and create a working neural net.\n",
    "Cost function is the thing mentioned in Andrew Ng's course (maybe re-watch fast.ai to get a refresser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRaw = pd.read_csv('Admission_Predict.csv')\n",
    "del dataRaw['Serial No.'] # Serves as a unique identifier, not needed for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRaw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Normalize it\n",
    "We use the min/max normalization equation:\n",
    "\n",
    "$$x_{new} = \\frac{x_{current}-X_{min}}{X_{max}-X_{min}},$$\n",
    "\n",
    "where $x_{current}$ is an element in column $X$ and $x_{new}$ is the updated value of $x_{current}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either use min max normalization \n",
    "normalizedDataRaw = (dataRaw-dataRaw.min())/(dataRaw.max()-dataRaw.min())\n",
    "# or mean std normalization.\n",
    "# normalizedDataRaw = (dataRaw-dataRaw.mean())/(dataRaw.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.663462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.599359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.730159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.451923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.492063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating    SOP   LOR       CGPA  \\\n",
       "0       0.94     0.928571               0.75  0.875  0.875  0.913462   \n",
       "1       0.68     0.535714               0.75  0.750  0.875  0.663462   \n",
       "2       0.52     0.428571               0.50  0.500  0.625  0.384615   \n",
       "3       0.64     0.642857               0.50  0.625  0.375  0.599359   \n",
       "4       0.48     0.392857               0.25  0.250  0.500  0.451923   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0       1.0          0.920635  \n",
       "1       1.0          0.666667  \n",
       "2       1.0          0.603175  \n",
       "3       1.0          0.730159  \n",
       "4       0.0          0.492063  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizedDataRaw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizedDataRaw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Set-Up forwards propagation\n",
    "* What are the dimensions of everything?\n",
    "\n",
    "High Level:\n",
    "\n",
    "Input Layer\n",
    "\n",
    "Hidden Layer 1, 10 linear nodes\n",
    "\n",
    "Hidden Layer 2, 10 linear nodes\n",
    "\n",
    "Hidden Layer 3, 10 linear nodes\n",
    "\n",
    "Hidden Layer 4, 10 linear nodes\n",
    "\n",
    "Hidden Layer 5, 10 linear nodes\n",
    "\n",
    "Sum, apply sigmoid? Whatever that cost function was.\n",
    "\n",
    "Dimensions:\n",
    "\n",
    "Input Layer:  [1x7]\n",
    "\n",
    "Hidden Layer 1: w: [7x10] b: [1x10]\n",
    "\n",
    "Hidden Layer 2: w: [10x10] b: [1x10]\n",
    "\n",
    "Hidden Layer 3: w: [10x10] b: [1x10]\n",
    "\n",
    "Hidden Layer 4: w: [10x10] b: [1x10]\n",
    "\n",
    "Hidden Layer 5: w: [10x10] b: [1x10]\n",
    "\n",
    "Let's get one forwards propagation. Don't worry about the cost function yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we do the weights? Just pull them from the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0395])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __call__(self, y, yhat):\n",
    "        self.y = y\n",
    "        self.yHat = yhat\n",
    "        tmp = (-1)*y*log(yhat) - (1-y)*log(1-yhat)\n",
    "        return tmp\n",
    "    \n",
    "    def backwards(self, grad):\n",
    "        '''\n",
    "        [(-y)/(yHat) - (1-y)/(1-yHat)]\n",
    "        '''\n",
    "        left = ((-1)*(self.y))/(self.yHat)\n",
    "        right = (1-self.y)/(1-self.yHat)\n",
    "        \n",
    "        grad *= (left-right)\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return 1/(1+exp((-1)*x)) \n",
    "    \n",
    "    def backwards(self, grad):\n",
    "        '''\n",
    "        (e^-x)/((1+e^-x)^2)\n",
    "        '''\n",
    "        top = exp((-1)*self.x)\n",
    "        bottom = (1+exp((-1)*self.x))**2\n",
    "        \n",
    "        grad *= top/bottom\n",
    "        \n",
    "        return grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.299581425007503e-24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Sigmoid\n",
    "sig = Sigmoid()\n",
    "\n",
    "sig(-55)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = LogisticRegression()\n",
    "# loss(5,5.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # m: Number of signals layer is receiving\n",
    "    # n: Number of nodes in layer\n",
    "    def __init__(self, m, n, alpha, wantBias=True):\n",
    "        self.w = torch.randn(m,n)\n",
    "        self.b = torch.zeros(1,n)\n",
    "        self.wantBias = wantBias\n",
    "        \n",
    "        if wantBias:  \n",
    "            self.b = torch.randn(1,n)\n",
    "\n",
    "\n",
    "        self.lr = alpha\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        \n",
    "        return inp@self.w + self.b\n",
    "   \n",
    "    def backwards(self, grad):\n",
    "        # Calculate new weight values\n",
    "        # 1) Add in a dimension so we can take transpose\n",
    "        # 2) Take transpose\n",
    "        # 3) Use broadcasting in order to have a matrix of proper size\n",
    "        \n",
    "        '''\n",
    "        dL/dX = (dL/dY)*wT\n",
    "        dL/dW = xT*(dL/dY)\n",
    "        '''\n",
    "        \n",
    "        if type(grad) != type([1,2]):\n",
    "            # We have a scalar\n",
    "            print(\"A\")\n",
    "            \n",
    "            # Don't think we need this, \n",
    "#             p = torch.unsqueeze(tmp[:-1],0) # tmp[:-1] represents our input\n",
    "            # We are assuming that our input only has one dimension.\n",
    "            print(self.inp)\n",
    "\n",
    "#             modelInput = torch.unsqueeze(self.inp,0)\n",
    "#             modelInput = torch.t(modelInput)\n",
    "            modelInput = self.inp.t()\n",
    "\n",
    "            # Just so I remember what this is:\n",
    "            #     w :=    w   - \\alpha*(gradient)\n",
    "            #     w :=    w   - \\alpha*(gradient of this layer)*(gradient of all of the layers)\n",
    "\n",
    "            self.nW = self.w.add_( (-1)*(self.lr)*modelInput*grad )\n",
    "\n",
    "            self.nB = self.b.add_( (-1)*(self.lr)*(grad) )\n",
    "            \n",
    "            return [grad,self.w.t()]\n",
    "            \n",
    "        elif type(grad) == type([1,2]):\n",
    "            # We don't have a scalar\n",
    "            print(\"B\")\n",
    "            \n",
    "            self.nW = self.w.add_((-1)*(self.lr)*grad[0]*grad[1])\n",
    "            self.nB = self.b.add_((-1)*(self.lr)*grad[0])\n",
    "            \n",
    "            return {\"grad\": grad, \"weights\": self.w, \"biases\": self.b}\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        self.w = self.nW\n",
    "        self.b = self.nB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = m.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.Tensor( [[5.6120, 4.8251, 1.8473, 1.7954, 2.9556]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6120],\n",
       "        [4.8251],\n",
       "        [1.8473],\n",
       "        [1.7954],\n",
       "        [2.9556]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, alpha):\n",
    "#         self.layers = [Linear(7,10,alpha), Linear(10,10,alpha), Linear(10,10,alpha), Linear(10,10,alpha), Linear(10,10,alpha), Linear(10,1,alpha)]\n",
    "#         self.layers = [Linear(7,10,alpha), Linear(10,10,alpha), Linear(10,1,alpha)]\n",
    "#         self.layers = [Linear(7,4,alpha), Linear(4,4,alpha), Linear(4,1,alpha)]\n",
    "        self.layers = [Linear(7,5,alpha), Linear(5,1,alpha, wantBias=False)]\n",
    "        self.activation = Sigmoid()\n",
    "        self.loss = LogisticRegression()\n",
    "        \n",
    "    def forwards(self, inp):\n",
    "        \n",
    "        y = inp[-1]\n",
    "        x = inp[:-1]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            print(\"x: {}\".format(x))\n",
    "            x = layer(x) \n",
    "            \n",
    "#         print(\"x: {}\".format(x))\n",
    "        \n",
    "        \n",
    "        yHat = self.activation(x)\n",
    "        \n",
    "        loss = self.loss(y, yHat)\n",
    "        \n",
    "        print(\"Loss is: {}\".format(loss))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def backwards(self):\n",
    "        self.grad = 1\n",
    "        \n",
    "        \n",
    "        expandedLayers = [self.loss, self.activation]\n",
    "        \n",
    "        for layer in expandedLayers:\n",
    "            self.grad = layer.backwards(self.grad)\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Iterate through expanded layers\n",
    "        # Then iterate through the reverse of linear layers\n",
    "        for layer in reversed(self.layers):\n",
    "            self.grad = layer.backwards(self.grad)\n",
    "            \n",
    "        # Built update into backwards\n",
    "        for layer in self.layers:\n",
    "            layer.update()\n",
    "        \n",
    "        \n",
    "        return self.grad\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = normalizedDataRaw.iloc[0,]\n",
    "# print(test)\n",
    "# print(\"\\n\")\n",
    "# print(test[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94      , 0.92857143, 0.75      , 0.875     , 0.875     ,\n",
       "       0.91346154, 1.        , 0.92063492])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.tensor(test.values, dtype=torch.float)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0.9400, 0.9286, 0.7500, 0.8750, 0.8750, 0.9135, 1.0000])\n",
      "x: tensor([[2.0027e-05, 6.2652e+00, 9.8404e-01, 1.5848e+00, 1.5815e+00]])\n",
      "Loss is: 0.27779802680015564\n"
     ]
    }
   ],
   "source": [
    "out = m.forwards(tmp)\n",
    "# out = m.forwards(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "tensor([[2.0027e-05, 6.2652e+00, 9.8404e-01, 1.5848e+00, 1.5815e+00]])\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "r = m.backwards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grad': tensor([[ 0.0246, -0.0348,  0.0541,  0.2520, -0.2374]]),\n",
       " 'weights': tensor([[-1.6635, -2.0175, -0.5783,  0.4368, -1.3694],\n",
       "         [ 1.0926, -0.2654,  0.7496,  1.3693,  0.7699],\n",
       "         [-2.0138,  0.0632, -0.2829,  0.2972, -2.6121],\n",
       "         [ 0.5067,  0.3938, -2.2581,  0.6780,  0.4267],\n",
       "         [-0.8548,  0.5003, -0.3730,  0.8662,  0.0191],\n",
       "         [ 1.2546,  0.0237, -1.0010, -1.9121,  2.3600],\n",
       "         [-1.2921, -0.5899, -0.1519, -0.2689, -0.1262]]),\n",
       " 'biases': tensor([[ 0.9318, -0.0039, -0.6942, -0.1002,  0.3415]])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "want to match up the columns\n",
    "so want to broadcast vertically and subtract\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = r['grad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = r['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = r['biases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3252, -1.0687,  1.0535, -1.8430, -0.4243],\n",
       "        [-0.0246,  1.6342,  0.5786, -0.0766, -0.0326],\n",
       "        [-1.2802,  0.4557, -0.1643,  0.5335, -1.6546],\n",
       "        [-2.4095,  0.6296,  1.5242, -1.0265,  0.4545],\n",
       "        [-0.4890,  1.4095,  0.2881,  0.2067,  0.5265],\n",
       "        [-0.7257,  0.7895, -0.1789, -1.7820,  0.6183],\n",
       "        [ 0.4492, -0.3642,  1.2904, -1.4778, -0.1929]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0143,  0.0045, -0.0455,  0.0644,  0.0288]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5105, -1.1277,  1.6445, -2.6800, -0.7991],\n",
       "        [-0.2099,  1.5752,  1.1696, -0.9137, -0.4074],\n",
       "        [-1.4655,  0.3967,  0.4267, -0.3035, -2.0294],\n",
       "        [-2.5948,  0.5705,  2.1152, -1.8635,  0.0796],\n",
       "        [-0.6743,  1.3505,  0.8791, -0.6304,  0.1516],\n",
       "        [-0.9111,  0.7305,  0.4121, -2.6191,  0.2435],\n",
       "        [ 0.2639, -0.4232,  1.8815, -2.3148, -0.5677]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.add_((-1)*grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9318, -0.0039, -0.6942, -0.1002,  0.3415]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9072,  0.0309, -0.7483, -0.3522,  0.5788]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.add_((-1)*grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
