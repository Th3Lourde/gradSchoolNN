{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRaw = pd.read_csv('Admission_Predict.csv')\n",
    "del dataRaw['Serial No.'] # Serves as a unique identifier, not needed for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRaw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either use min max normalization \n",
    "normalizedDataRaw = (dataRaw-dataRaw.min())/(dataRaw.max()-dataRaw.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.663462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.599359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.730159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.451923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.492063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating    SOP   LOR       CGPA  \\\n",
       "0       0.94     0.928571               0.75  0.875  0.875  0.913462   \n",
       "1       0.68     0.535714               0.75  0.750  0.875  0.663462   \n",
       "2       0.52     0.428571               0.50  0.500  0.625  0.384615   \n",
       "3       0.64     0.642857               0.50  0.625  0.375  0.599359   \n",
       "4       0.48     0.392857               0.25  0.250  0.500  0.451923   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0       1.0          0.920635  \n",
       "1       1.0          0.666667  \n",
       "2       1.0          0.603175  \n",
       "3       1.0          0.730159  \n",
       "4       0.0          0.492063  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizedDataRaw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk =  np.random.rand(len(normalizedDataRaw)) < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = normalizedDataRaw[msk]\n",
    "testData = normalizedDataRaw[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __call__(self, y, yhat):\n",
    "        self.y = y\n",
    "        self.yHat = yhat\n",
    "        tmp = (-1)*y*log(max(yhat, 0.000000000000001)) - (1-y)*log(max(1-yhat,0.000000000000001))\n",
    "        return tmp\n",
    "    \n",
    "    def backwards(self, grad):\n",
    "        '''\n",
    "        [(-y)/(yHat) - (1-y)/(1-yHat)]\n",
    "        '''\n",
    "        left = ((-1)*(self.y))/(self.yHat)\n",
    "        right = (1-self.y)/(1-self.yHat)\n",
    "        \n",
    "        grad *= (left-right)\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return 1/(1+exp((-1)*x)) \n",
    "    \n",
    "    def backwards(self, grad):\n",
    "        '''\n",
    "        (e^-x)/((1+e^-x)^2)\n",
    "        '''\n",
    "        top = exp((-1)*self.x)\n",
    "        bottom = (1+exp((-1)*self.x))**2\n",
    "        \n",
    "        grad *= top/bottom\n",
    "        \n",
    "        return grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    # m: Number of signals layer is receiving\n",
    "    # n: Number of nodes in layer\n",
    "    def __init__(self, m, n, alpha, wantBias=True):\n",
    "        self.w = torch.randn(m,n)\n",
    "        self.b = torch.zeros(1,n)\n",
    "        self.wantBias = wantBias\n",
    "        \n",
    "        if wantBias:  \n",
    "            self.b = torch.randn(1,n)\n",
    "\n",
    "\n",
    "        self.lr = alpha\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        \n",
    "        return inp@self.w + self.b\n",
    "   \n",
    "    def backwards(self, grad):\n",
    "        # Calculate new weight values\n",
    "        # 1) Add in a dimension so we can take transpose\n",
    "        # 2) Take transpose\n",
    "        # 3) Use broadcasting in order to have a matrix of proper size\n",
    "        \n",
    "        '''\n",
    "        dL/dX = (dL/dY)*wT\n",
    "        dL/dW = xT*(dL/dY)\n",
    "        '''\n",
    "        \n",
    "        if type(grad) != type([1,2]):\n",
    "            modelInput = self.inp.t()\n",
    "\n",
    "            # Just so I remember what this is:\n",
    "            #     w :=    w   - \\alpha*(gradient)\n",
    "            #     w :=    w   - \\alpha*(gradient of this layer)*(gradient of all of the layers)\n",
    "\n",
    "            self.nW = self.w.add_( (-1)*(self.lr)*modelInput*grad )\n",
    "            self.nB = self.b.add_( (-1)*(self.lr)*(grad) )\n",
    "            \n",
    "            return [grad,self.w.t()]\n",
    "            \n",
    "        elif type(grad) == type([1,2]):\n",
    "            # We don't have a scalar\n",
    "            \n",
    "            self.nW = self.w.add_((-1)*(self.lr)*grad[0]*grad[1])\n",
    "            self.nB = self.b.add_((-1)*(self.lr)*grad[0]*grad[1])\n",
    "            \n",
    "            # [grad,self.w.t()] \n",
    "            grad[0] = grad[0]*grad[1]\n",
    "            grad[1] = self.w.t()\n",
    "            \n",
    "            return grad\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        self.w = self.nW\n",
    "        self.b = self.nB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, alpha):\n",
    "#         self.layers = [Linear(7,10,alpha), Linear(10,10,alpha), Linear(10,10,alpha), Linear(10,10,alpha), Linear(10,10,alpha), Linear(10,1,alpha)]\n",
    "#         self.layers = [Linear(7,10,alpha), Linear(10,10,alpha), Linear(10,1,alpha)]\n",
    "#         self.layers = [Linear(7,4,alpha), Linear(4,4,alpha), Linear(4,1,alpha)]\n",
    "#         self.layers = [Linear(7,15,alpha), Linear(15,1,alpha, wantBias=False)]\n",
    "        self.layers = [Linear(7,50,alpha), Linear(50,1,alpha, wantBias=False)]\n",
    "#         self.layers = [Linear(7,5,alpha), Linear(5,1,alpha, wantBias=False)]\n",
    "        self.activation = Sigmoid()\n",
    "        self.loss = LogisticRegression()\n",
    "        \n",
    "    def forwards(self, inp):\n",
    "        \n",
    "        y = inp[-1]\n",
    "        x = inp[:-1]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x) \n",
    "        \n",
    "        yHat = self.activation(x)\n",
    "        \n",
    "#         print(\"[yHat]: {}\".format(yHat))\n",
    "        \n",
    "        \n",
    "        loss = self.loss(y, yHat)\n",
    "        \n",
    "#         print(\"Loss is: {}\".format(loss))\n",
    "        \n",
    "        return loss \n",
    "    \n",
    "    def backwards(self):\n",
    "        self.grad = 1\n",
    "        \n",
    "        \n",
    "        expandedLayers = [self.loss, self.activation]\n",
    "        \n",
    "        for layer in expandedLayers:\n",
    "            self.grad = layer.backwards(self.grad)\n",
    "            \n",
    "        # Iterate through expanded layers\n",
    "        # Then iterate through the reverse of linear layers\n",
    "        for layer in reversed(self.layers):\n",
    "            self.grad = layer.backwards(self.grad)\n",
    "            \n",
    "        # Built update into backwards\n",
    "        for layer in self.layers:\n",
    "            layer.update()\n",
    "        \n",
    "        return self.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(0.0000000001)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[loss for epoch 1] 1.5436700582504272\n",
      "[loss for epoch 2] 3.08733868598938\n",
      "[loss for epoch 3] 4.631008625030518\n",
      "[loss for epoch 4] 6.174678325653076\n",
      "[loss for epoch 5] 7.718349456787109\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i in range(len(trainData)):\n",
    "        train = trainData.iloc[i,]\n",
    "        train = torch.tensor(train.values, dtype=torch.float)\n",
    "        loss += m.forwards(train)\n",
    "        m.backwards()\n",
    "        \n",
    "        \n",
    "    print(\"[loss for epoch {}] {}\".format(epoch+1, loss/len(trainData)))\n",
    "    \n",
    "# print(\"[ave loss] {}\".format(loss/len(trainData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0.9400, 0.9286, 0.7500, 0.8750, 0.8750, 0.9135, 1.0000])\n",
      "x: tensor([[2.0027e-05, 6.2652e+00, 9.8404e-01, 1.5848e+00, 1.5815e+00]])\n",
      "Loss is: 0.27779802680015564\n"
     ]
    }
   ],
   "source": [
    "# out = m.forwards(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "tensor([[2.0027e-05, 6.2652e+00, 9.8404e-01, 1.5848e+00, 1.5815e+00]])\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "r = m.backwards()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
